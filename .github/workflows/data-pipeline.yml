name: Data Pipeline - Scheduled Run

on:
  schedule:
    - cron: '0 */2 * * *'  # Every 2 hours
  workflow_dispatch:  # Manual trigger

jobs:
  run-pipeline:
    runs-on: ubuntu-latest
    permissions:
      contents: write  # Allow pushing commits
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r dataset/requirements.txt
      
      - name: Configure Git
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
      
      - name: Run data pipeline
        env:
          OPENAQ_API_KEY: ${{ secrets.OPENAQ_API_KEY }}
        run: |
          cd dataset
          python incremental_loader.py --locations 3459 5506835 5509787 6093549 6093550 6093551 6133623 6142022 6142174 6142175 || exit 0
      
      - name: Commit and push changes
        run: |
          git add -f dataset/location_*.json || true
          git add -f dataset/.state.json || true
          
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "chore: update data from scheduled pipeline run [skip ci]"
            git push
          fi
      
      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-logs-${{ github.run_number }}
          path: |
            dataset/pipeline.log
            dataset/metrics.json
          if-no-files-found: ignore
          retention-days: 7
