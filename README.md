# Self-Healing MLOps: Automated Air Quality Data Pipeline

> **A production-ready data pipeline that automatically collects air quality measurements every 2 hours, validates data quality, and maintains a continuously growing dataset for ML model training.**

![System Architecture](Image/archietecture.png)

---

## ğŸ“– Table of Contents

- [What Does This Do?](#-what-does-this-do)
- [Quick Start](#-quick-start-3-steps)
- [How It Works](#-how-it-works)
- [What I Built](#-what-i-built)
- [Project Structure](#-project-structure)
- [Technical Details](#-technical-details)
- [Next Steps](#-next-steps)

---

## ğŸ¯ What Does This Do?

This project automatically collects real-time air quality data from sensors around the world and prepares it for machine learning. Think of it as a **self-running data collection robot** that:

1. **Fetches fresh air quality data** every 2 hours from OpenAQ API
2. **Stores it in JSON files** (one file per location)
3. **Validates data quality** (catches bad/missing data)
4. **Removes duplicates** automatically
5. **Tracks progress** (knows what data it already has)
6. **Runs forever** without manual intervention

### Real-World Example:

```
12:00 PM â†’ Fetch new air quality data â†’ Add 8 new measurements
2:00 PM  â†’ Fetch new air quality data â†’ Add 12 new measurements
4:00 PM  â†’ Fetch new air quality data â†’ Add 10 new measurements
... continues every 2 hours, 24/7 ...
```

Your dataset keeps growing automatically! ğŸ“ˆ

---

## ğŸš€ Quick Start (3 Steps)

### Step 1: Setup Environment
```bash
# Clone the repository
git clone https://github.com/imarobot00/Self_healing_MLOPS.git
cd Self_healing_MLOPS

# Create environment file
cp dataset/.env.example dataset/.env
```

### Step 2: Start the Pipeline
```bash
# Option A: Using Docker (recommended)
docker compose up -d

# Option B: Run locally
cd dataset
pip install -r requirements.txt
python scheduler.py --mode interval --interval 2
```

### Step 3: Verify It's Working
```bash
# Watch it collect data
docker compose logs -f data-pipeline

# Check your data files (they'll appear soon)
ls -lh dataset/location_*.json
```

**That's it!** The pipeline now runs automatically every 2 hours. ğŸ‰

---

## ğŸ” How It Works

### The Logic Behind the Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EVERY 2 HOURS                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Load State File (.state.json)                          â”‚
â”‚     "What data do I already have?"                          â”‚
â”‚     Example: Last fetch was 2025-12-05 12:00:00            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Query OpenAQ API                                        â”‚
â”‚     "Give me measurements AFTER 2025-12-05 12:00:00"        â”‚
â”‚     Fetches: PM2.5, PM10, Temperature, Humidity, etc.       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Load Existing Data                                      â”‚
â”‚     Opens: location_3459.json (1000 existing records)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Remove Duplicates                                       â”‚
â”‚     New: 10 records â†’ Check against existing â†’ Keep 8 new   â”‚
â”‚     (2 were duplicates, removed)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. Validate Data Quality                                   â”‚
â”‚     âœ“ PM2.5 in range [0-1000]?                             â”‚
â”‚     âœ“ Temperature reasonable?                               â”‚
â”‚     âœ“ Timestamp format correct?                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. Append to File                                          â”‚
â”‚     location_3459.json: 1000 + 8 = 1008 records            â”‚
â”‚     File size grows: 25MB â†’ 25.2MB                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. Update State                                            â”‚
â”‚     .state.json: "Last fetch: 2025-12-05 14:00:00"         â”‚
â”‚     Ready for next run in 2 hours!                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why This Design?

**Incremental Loading:** Only fetches new data (efficient, saves API calls)  
**State Tracking:** Remembers progress (no data loss on restart)  
**Deduplication:** Prevents duplicate records (data quality)  
**Validation:** Catches bad data early (reliability)  
**Automation:** Runs forever without human intervention (scalability)

---

## ğŸ› ï¸ What I Built


### Core Components (Built & Working âœ…)

#### 1. **Incremental Data Loader** (`dataset/incremental_loader.py`)
- Fetches air quality measurements from OpenAQ API
- Only downloads NEW data (incremental loading)
- Deduplicates records automatically
- Appends to existing JSON files
- Tracks state in `.state.json`

#### 2. **Scheduler** (`dataset/scheduler.py`)
- Runs the loader every 2 hours automatically
- Uses APScheduler (Python scheduling library)
- Handles errors gracefully (retries on failure)
- Logs all activities
- Supports interval or cron-based scheduling

#### 3. **Data Validator** (`dataset/validator.py`)
- Checks data quality (schema validation)
- Verifies value ranges (e.g., PM2.5: 0-1000 Âµg/mÂ³)
- Validates timestamps
- Generates quality reports
- Calculates quality scores

#### 4. **Monitoring System** (`dataset/monitor.py`)
- Structured logging (console + file)
- Metrics collection (records fetched, errors, duration)
- Alert manager (Slack, email, webhooks)
- Tracks consecutive failures
- Historical metrics storage

#### 5. **Docker Deployment**
- `Dockerfile` - Container definition
- `docker-compose.yml` - Service orchestration
- Automatic restarts on failure
- Volume mounts for data persistence
- Health checks

#### 6. **GitHub Actions CI/CD** (`.github/workflows/data-pipeline.yml`)
- Runs every 2 hours in GitHub's cloud
- Commits updated data back to repo
- Completely free and serverless
- Alternative to local Docker

---

## ğŸ“ Project Structure

```
Self_healing_MLOPS/
â”‚
â”œâ”€â”€ dataset/                          # ğŸ¯ Main data pipeline
â”‚   â”œâ”€â”€ incremental_loader.py        # Fetches & updates data
â”‚   â”œâ”€â”€ scheduler.py                 # Runs every 2 hours
â”‚   â”œâ”€â”€ validator.py                 # Data quality checks
â”‚   â”œâ”€â”€ monitor.py                   # Logging & alerts
â”‚   â”œâ”€â”€ config.yaml                  # Configuration
â”‚   â”œâ”€â”€ requirements.txt             # Python dependencies
â”‚   â”œâ”€â”€ .env                         # API keys (you create this)
â”‚   â”œâ”€â”€ .state.json                  # State tracking (auto-created)
â”‚   â”œâ”€â”€ location_*.json              # Data files (auto-created)
â”‚   â””â”€â”€ PIPELINE_README.md           # Detailed documentation
â”‚
â”œâ”€â”€ Dockerfile                        # Container definition
â”œâ”€â”€ docker-compose.yml                # Service orchestration
â”œâ”€â”€ quickstart.sh                     # Easy start script
â”‚
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ data-pipeline.yml             # GitHub Actions (cloud automation)
â”‚
â”œâ”€â”€ Image/
â”‚   â””â”€â”€ archietecture.png             # System architecture diagram
â”‚
â”œâ”€â”€ README.md                         # â† You are here!
â”œâ”€â”€ CHEATSHEET.md                     # Command reference
â””â”€â”€ IMPLEMENTATION_SUMMARY.md         # Technical details
```

---

## ğŸ”§ Technical Details

### Technologies Used

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Language** | Python 3.11+ | Core programming |
| **API** | OpenAQ v3 | Air quality data source |
| **Scheduler** | APScheduler | Automated execution |
| **Container** | Docker | Deployment & isolation |
| **CI/CD** | GitHub Actions | Cloud automation |
| **Config** | YAML | Configuration management |
| **Logging** | Python logging | Activity tracking |
| **Data Format** | JSON | Structured data storage |

### Data Flow Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OpenAQ API â”‚  (Global air quality sensors)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTPS REST API
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Incremental      â”‚  Every 2 hours
â”‚ Loader           â”‚  - Fetches new data only
â”‚                  â”‚  - Pagination handling
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Deduplicator    â”‚  Removes duplicates
â”‚                  â”‚  (composite key matching)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Validator      â”‚  Quality checks
â”‚                  â”‚  - Schema validation
â”‚                  â”‚  - Range validation
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  JSON Storage    â”‚  location_*.json files
â”‚  + State         â”‚  .state.json
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How Incremental Loading Works

**Traditional Approach (Inefficient):**
```
Every run â†’ Download ALL data â†’ 1M records â†’ 50MB â†’ Slow
```

**Our Approach (Smart):**
```
First run  â†’ Download all      â†’ 1,000 records â†’ Save state: "2025-12-05 12:00"
Second run â†’ Download since 12:00 â†’ 8 new records â†’ Append
Third run  â†’ Download since 14:00 â†’ 12 new records â†’ Append
```

**Result:** 100x faster, less API usage, no duplicates!

### Deduplication Algorithm

```python
# Each record gets a unique key
key = f"{location_id}_{parameter_id}_{sensor_id}_{timestamp_from}_{timestamp_to}"

# Example:
# "3459_pm25_12345_2025-12-05T12:00:00Z_2025-12-05T13:00:00Z"

# Before adding: check if key exists â†’ Skip if yes â†’ Add if no
```

---

## ğŸ“ What You Can Do With This

### Current Capabilities (Phase 1 - Complete âœ…)

- âœ… **Collect data automatically** every 2 hours
- âœ… **9 locations monitored** (configurable in `config.yaml`)
- âœ… **Data validation** and quality checks
- âœ… **Historical data** preserved and growing
- âœ… **Run locally or in cloud** (Docker/GitHub Actions)
- âœ… **Monitoring and alerts** for failures

### Next Steps (Phase 2 - Future Work)

- [ ] **ML Model Training**: Train LSTM/Regression on collected data
- [ ] **Feature Engineering**: Create rolling averages, time features
- [ ] **Drift Detection**: Implement KS-test, PSI for data drift
- [ ] **FastAPI Service**: Real-time predictions endpoint
- [ ] **Grafana Dashboard**: Visualize data and metrics
- [ ] **Azure VM Deployment**: Production hosting
- [ ] **Self-Healing Logic**: Auto-retrain on drift detection

---

## ğŸ“Š Sample Data Structure

### What the Data Looks Like

**location_3459.json:**
```json
[
  {
    "value": 25.3,
    "parameter": {
      "id": 19,
      "name": "pm25",
      "units": "Âµg/mÂ³"
    },
    "period": {
      "interval": "01:00:00",
      "datetimeFrom": {
        "utc": "2025-12-05T12:00:00Z"
      },
      "datetimeTo": {
        "utc": "2025-12-05T13:00:00Z"
      }
    },
    "locationId": 3459,
    "sensors": [
      {
        "id": 12345,
        "name": "AirQuality Sensor #1"
      }
    ]
  },
  ... thousands more records ...
]
```

**State File (.state.json):**
```json
{
  "locations": {
    "3459": {
      "last_fetch_time": "2025-12-05T14:00:00Z",
      "last_records_count": 8,
      "last_successful_run": "2025-12-05T14:05:23Z"
    }
  }
}
```

---

## ğŸ® Common Commands

```bash
# Start the pipeline
docker compose up -d

# Watch logs (see it work in real-time)
docker compose logs -f data-pipeline

# Stop the pipeline
docker compose down

# Check data files
ls -lh dataset/location_*.json

# View state
cat dataset/.state.json | python -m json.tool

# Run once (testing)
cd dataset && python incremental_loader.py --locations 3459

# Test components
cd dataset && python test_pipeline.py
```

---

## ğŸ› Troubleshooting

### Pipeline Not Running?
```bash
# Check container status
docker compose ps

# View logs for errors
docker compose logs data-pipeline
```

### No Data Being Collected?
```bash
# Check API connectivity
curl "https://api.openaq.org/v3/locations/3459"

# Verify .env file exists
cat dataset/.env
```

### Docker Issues?
```bash
# Restart Docker service
sudo systemctl restart docker

# Rebuild container
docker compose up -d --build
```

---

## ğŸš€ Deployment Options

### Option 1: Local Docker (Current Setup)
- **Runs on:** Your computer
- **Cost:** Free
- **Pros:** Full control, instant access
- **Cons:** Only runs when computer is on

### Option 2: GitHub Actions (Cloud, Free)
- **Runs on:** GitHub's servers
- **Cost:** Free (public repos)
- **Pros:** 24/7, no local resources
- **Cons:** 6hr/month limit, data in repo

### Option 3: Azure VM (Production)

```
self-healing-air-quality/
â”‚
â”œâ”€â”€ dataset/              # ğŸ†• Automated data pipeline (implemented)
â”‚   â”œâ”€â”€ incremental_loader.py   # Fetch & update data every 2 hours
â”‚   â”œâ”€â”€ scheduler.py             # APScheduler for automation
â”‚   â”œâ”€â”€ validator.py             # Data quality validation
â”‚   â”œâ”€â”€ monitor.py               # Logging & metrics
â”‚   â”œâ”€â”€ config.yaml              # Pipeline configuration
â”‚   â”œâ”€â”€ location_*.json          # Air quality data files
â”‚   â””â”€â”€ PIPELINE_README.md       # ğŸ“– Full pipeline documentation
â”‚
â”œâ”€â”€ data/                 # Raw + processed datasets (future)
â”œâ”€â”€ models/               # Saved model artifacts (future)
â”œâ”€â”€ src/                  # ML pipeline (to be implemented)
â”‚   â”œâ”€â”€ training/         # Training & retraining scripts
â”‚   â”œâ”€â”€ inference/        # FastAPI app
â”‚   â”œâ”€â”€ monitoring/       # Drift detection, alerts
â”‚   â””â”€â”€ utils/            # Shared utilities
â”‚
â”œâ”€â”€ docker/               # ğŸ†• Containerization (implemented)
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ .github/              # ğŸ†• CI/CD workflows (implemented)
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ data-pipeline.yml   # Scheduled data loading
â”‚
â”œâ”€â”€ Image/
â”‚   â””â”€â”€ archietecture.png  # System architecture diagram
â”‚
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start - Data Pipeline

The automated data pipeline is now fully operational! It fetches OpenAQ air quality data every 2 hours.

### Option 1: Docker (Recommended)
```bash
# Setup
cp dataset/.env.example dataset/.env
# Add your OPENAQ_API_KEY to .env (optional)

# Run
docker-compose up -d

- **Runs on:** Azure cloud servers
- **Cost:** ~$10-20/month
- **Pros:** 24/7, production-grade, scalable
- **Cons:** Requires setup, monthly cost

---

## ğŸ“š Additional Documentation

- **[PIPELINE_README.md](dataset/PIPELINE_README.md)** - Complete pipeline documentation
- **[CHEATSHEET.md](CHEATSHEET.md)** - Quick command reference
- **[IMPLEMENTATION_SUMMARY.md](IMPLEMENTATION_SUMMARY.md)** - Technical deep dive

---

## ğŸ’¡ Key Learnings & Design Decisions

### Why Incremental Loading?
Fetching only new data is 100x faster than downloading everything each time. Saves API quota and bandwidth.

### Why JSON Files?
- Simple, human-readable
- No database setup needed
- Easy to version control
- Perfect for development
- Later: migrate to PostgreSQL/TimescaleDB for production

### Why State Tracking?
Without state, we'd have to scan all existing data to find what's new. State file makes it instant.

### Why Docker?
- Same environment everywhere
- Easy deployment
- Isolation from system
- One command to start/stop

### Why Every 2 Hours?
Balance between freshness and API limits. Too frequent = wasted calls. Too long = stale data.

---

## ğŸ¯ Success Metrics

After running for 1 week, you should have:
- âœ… ~84 automatic data fetches (24/7 Ã— 7 days Ã· 2 hours)
- âœ… ~1000+ new measurements per location
- âœ… Validated, deduplicated, clean dataset
- âœ… Ready for ML model training

---

## ğŸ¤ Contributing

This is a personal project, but feel free to:
- Open issues for bugs
- Suggest improvements
- Fork and extend for your use case

---

## ğŸ“ License

MIT License - feel free to use for learning and personal projects.

---

## ğŸ‘¤ Author

**Bipul Dahal**  
Final-year Engineering Student  
Building production-grade MLOps systems

ğŸ“§ Contact: [GitHub - @imarobot00](https://github.com/imarobot00)

---

## ğŸ™ Acknowledgments

- **OpenAQ** - For providing free air quality data API
- **APScheduler** - Python scheduling library
- **Docker** - Containerization platform
- **GitHub Actions** - Free CI/CD automation

---

## â­ Quick Reference

| Task | Command |
|------|---------|
| Start pipeline | `docker compose up -d` |
| View logs | `docker compose logs -f data-pipeline` |
| Stop pipeline | `docker compose down` |
| Check data | `ls -lh dataset/location_*.json` |
| Test components | `cd dataset && python test_pipeline.py` |
| Run once | `./quickstart.sh once` |

---

**Built with â¤ï¸ for the MLOps community**
